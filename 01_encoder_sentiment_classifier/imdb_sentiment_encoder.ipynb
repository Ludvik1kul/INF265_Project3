{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install tokenizers\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "MwuC9YlY_Zwy",
        "outputId": "ade055eb-92db-40e0-f869-2dc945fe6cdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lFmtUrfB9tGU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import os\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Avoids warning from tokenizer when using num_workers > 0 in DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-H8SGuc9tGV"
      },
      "source": [
        "# Encoder-only Transformer for Text Sentiment Classification\n",
        "\n",
        "## 1. Data\n",
        "\n",
        "### 1.1 Load the IMDB dataset\n",
        "\n",
        "We will use the IMDB dataset for sentiment classification. The dataset consists of 50,000 movie reviews, each labeled as positive or negative. We will use 25k reviews for training, 5k for validation, and 20k for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4mSnMJ019tGX",
        "outputId": "30084e93-8da2-403c-ab8c-0dab29f8005a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 25000\n",
            "Validation size: 5000\n",
            "Test size: 20000\n"
          ]
        }
      ],
      "source": [
        "# Constants\n",
        "SEED = 420\n",
        "VAL_SIZE = 0.2\n",
        "\n",
        "# Load IMDB dataset\n",
        "dataset_train = load_dataset(\"imdb\", split=\"train\")\n",
        "dataset_test = load_dataset(\"imdb\", split=\"test\")\n",
        "\n",
        "# Split test into test and validation\n",
        "dataset_test = dataset_test.train_test_split(test_size=1-VAL_SIZE, seed=SEED)\n",
        "dataset_test, dataset_val = dataset_test[\"test\"], dataset_test[\"train\"]\n",
        "\n",
        "print(f\"Train size: {len(dataset_train)}\")\n",
        "print(f\"Validation size: {len(dataset_val)}\")\n",
        "print(f\"Test size: {len(dataset_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SzRh3LH9tGX"
      },
      "source": [
        "### 1.2 Preprocess the data\n",
        "\n",
        "We remove HTML tags, special characters, and convert the text to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AKxn1Zvn9tGX"
      },
      "outputs": [],
      "source": [
        "def remove_html_tags(text):\n",
        "    # Remove HTML tags\n",
        "    return re.sub(r'<[^>]*>', '', text)\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    # Remove special characters except for ,.!? and space\n",
        "    return re.sub(r'[^a-zA-Z0-9.,!? ]', '', text)\n",
        "\n",
        "def to_lowercase(text):\n",
        "    # Convert text to lowercase\n",
        "    return text.lower()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Apply all preprocessing steps to the text\n",
        "    text = remove_html_tags(text)\n",
        "    text = remove_special_characters(text)\n",
        "    text = to_lowercase(text)\n",
        "    return text\n",
        "\n",
        "def preprocess_batch(examples):\n",
        "    # Apply preprocessing to all texts in the batch\n",
        "    examples[\"text\"] = [preprocess_text(text) for text in examples[\"text\"]]\n",
        "    return examples\n",
        "\n",
        "# Preprocess the dataset\n",
        "dataset_train = dataset_train.map(preprocess_batch, batched=True)\n",
        "dataset_val = dataset_val.map(preprocess_batch, batched=True)\n",
        "dataset_test = dataset_test.map(preprocess_batch, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdID6BcT9tGY"
      },
      "source": [
        "### 1.3 Tokenization\n",
        "\n",
        "We use a simple word-level tokenizer to tokenize the text data. We use three special tokens: `[PAD]`, `[UNK]` and `[CLS]`. The `[PAD]` token will be used to pad the input sequences to the same length. The `[UNK]` token is used to represent out-of-vocabulary words (rare words). When classifying text, we will prepend the `[CLS]` token to the input sequence and use its output as the representation of the whole sequence.\n",
        "\n",
        "To reduce the vocabulary size, we only keep words appearing at least `MIN_FREQUENCY` times in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bB8q_fbw9tGY"
      },
      "outputs": [],
      "source": [
        "# Special tokens (padding, unknown, classification)\n",
        "PAD_TOKEN = \"[PAD]\"\n",
        "UNK_TOKEN = \"[UNK]\"\n",
        "CLS_TOKEN = \"[CLS]\"\n",
        "MIN_FREQUENCY = 10\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))\n",
        "tokenizer.pre_tokenizer = Whitespace() # Split on whitespace\n",
        "\n",
        "# Train tokenizer on the training set\n",
        "trainer = WordLevelTrainer(special_tokens=[PAD_TOKEN, UNK_TOKEN, CLS_TOKEN], min_frequency=MIN_FREQUENCY, vocab_size=VOCAB_SIZE)\n",
        "tokenizer.train_from_iterator(dataset_train[\"text\"], trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWxbkJQE9tGY"
      },
      "source": [
        "We test the tokenizer on a few examples to verify that it works as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QWf3lzMM9tGY",
        "outputId": "25ef41f3-43f8-4189-c1b3-6339b775a760",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: i rented i am curiousyellow from my video store because of all the controversy that surrounded it when it was first released in 1967. i also heard that at first it was seized by u.s. customs if it ever tried to enter this country, therefore being a fan of films considered controversial i really had to see this for myself.the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life. in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.what kills me about i am curiousyellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then its not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman, arguably their answer to good old boy john ford, had sex scenes in his films.i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america. i am curiousyellow is a good film for anyone wanting to study the meat and potatoes no pun intended of swedish cinema. but really, this film doesnt have much of a plot.\n",
            "Tokenized text (tokens): ['i', 'rented', 'i', 'am', '[UNK]', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967', '.', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', '[UNK]', 'by', 'u', '.', 's', '.', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', ',', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', 'controversial', 'i', 'really', 'had', 'to', 'see', 'this', 'for', 'myself', '.', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her', '[UNK]', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', '[UNK]', 'thought', 'about', 'certain', 'political', 'issues', 'such', 'as', 'the', 'vietnam', 'war', 'and', 'race', 'issues', 'in', 'the', 'united', 'states', '.', 'in', 'between', 'asking', 'politicians', 'and', 'ordinary', '[UNK]', 'of', '[UNK]', 'about', 'their', 'opinions', 'on', 'politics', ',', 'she', 'has', 'sex', 'with', 'her', 'drama', 'teacher', ',', 'classmates', ',', 'and', 'married', 'men', '.', 'what', 'kills', 'me', 'about', 'i', 'am', '[UNK]', 'is', 'that', '40', 'years', 'ago', ',', 'this', 'was', 'considered', 'pornographic', '.', 'really', ',', 'the', 'sex', 'and', 'nudity', 'scenes', 'are', 'few', 'and', 'far', 'between', ',', 'even', 'then', 'its', 'not', 'shot', 'like', 'some', 'cheaply', 'made', 'porno', '.', 'while', 'my', '[UNK]', 'mind', 'find', 'it', 'shocking', ',', 'in', 'reality', 'sex', 'and', 'nudity', 'are', 'a', 'major', '[UNK]', 'in', 'swedish', 'cinema', '.', 'even', '[UNK]', 'bergman', ',', 'arguably', 'their', 'answer', 'to', 'good', 'old', 'boy', 'john', 'ford', ',', 'had', 'sex', 'scenes', 'in', 'his', 'films', '.', 'i', 'do', '[UNK]', 'the', 'filmmakers', 'for', 'the', 'fact', 'that', 'any', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purposes', 'rather', 'than', 'just', 'to', 'shock', 'people', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornographic', 'theaters', 'in', 'america', '.', 'i', 'am', '[UNK]', 'is', 'a', 'good', 'film', 'for', 'anyone', 'wanting', 'to', 'study', 'the', 'meat', 'and', '[UNK]', 'no', 'pun', 'intended', 'of', 'swedish', 'cinema', '.', 'but', 'really', ',', 'this', 'film', 'doesnt', 'have', 'much', 'of', 'a', 'plot', '.']\n",
            "Tokenized text (IDs): [13, 1577, 13, 242, 1, 39, 60, 396, 1144, 89, 8, 33, 3, 7116, 15, 3369, 12, 55, 12, 16, 91, 628, 11, 7200, 4, 13, 83, 553, 15, 34, 91, 12, 16, 1, 35, 1242, 4, 826, 4, 9091, 46, 12, 127, 783, 9, 2501, 14, 705, 5, 1590, 114, 7, 338, 8, 101, 1168, 3060, 13, 67, 72, 9, 70, 14, 18, 536, 4, 3, 118, 10, 5920, 189, 7, 187, 3887, 473, 1485, 769, 4641, 38, 483, 9, 839, 283, 58, 73, 45, 123, 4, 11, 831, 58, 483, 9, 1140, 42, 1, 9, 252, 49, 436, 8, 678, 23, 51, 3, 870, 1, 205, 45, 784, 1011, 1314, 142, 17, 3, 2698, 332, 6, 1530, 1314, 11, 3, 2378, 1575, 4, 11, 199, 2188, 6981, 6, 1929, 1, 8, 1, 45, 69, 4644, 23, 2412, 5, 58, 47, 395, 19, 42, 473, 1750, 5, 7863, 5, 6, 1007, 351, 4, 51, 1074, 74, 45, 13, 242, 1, 10, 15, 1816, 153, 588, 5, 14, 16, 1168, 8190, 4, 67, 5, 3, 395, 6, 1010, 141, 26, 169, 6, 239, 199, 5, 59, 97, 32, 24, 322, 41, 49, 6632, 96, 4560, 4, 137, 60, 1, 346, 167, 12, 1586, 5, 11, 635, 395, 6, 1010, 26, 7, 662, 1, 11, 3887, 444, 4, 59, 1, 4940, 5, 4583, 69, 1486, 9, 53, 170, 450, 308, 2078, 5, 72, 395, 141, 11, 27, 101, 4, 13, 87, 1, 3, 888, 18, 3, 193, 15, 105, 395, 612, 11, 3, 22, 10, 612, 18, 1585, 5017, 245, 76, 44, 9, 1490, 86, 6, 102, 279, 9, 30, 612, 11, 8190, 2161, 11, 927, 4, 13, 242, 1, 10, 7, 53, 22, 18, 254, 1755, 9, 2066, 3, 3768, 6, 1, 61, 5219, 1409, 8, 3887, 444, 4, 21, 67, 5, 14, 22, 151, 28, 78, 8, 7, 118, 4]\n",
            "Decoded text: i rented i am [UNK] from my video store because of all the controversy that surrounded it when it was first released in 1967 . i also heard that at first it was [UNK] by u . s . customs if it ever tried to enter this country , therefore being a fan of films considered controversial i really had to see this for myself . the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life . in particular she wants to focus her [UNK] to making some sort of documentary on what the average [UNK] thought about certain political issues such as the vietnam war and race issues in the united states . in between asking politicians and ordinary [UNK] of [UNK] about their opinions on politics , she has sex with her drama teacher , classmates , and married men . what kills me about i am [UNK] is that 40 years ago , this was considered pornographic . really , the sex and nudity scenes are few and far between , even then its not shot like some cheaply made porno . while my [UNK] mind find it shocking , in reality sex and nudity are a major [UNK] in swedish cinema . even [UNK] bergman , arguably their answer to good old boy john ford , had sex scenes in his films . i do [UNK] the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america . i am [UNK] is a good film for anyone wanting to study the meat and [UNK] no pun intended of swedish cinema . but really , this film doesnt have much of a plot .\n",
            "\n",
            "Original text: i am curious yellow is a risible and pretentious steaming pile. it doesnt matter what ones political views are because this film can hardly be taken seriously on any level. as for the claim that frontal male nudity is an automatic nc17, that isnt true. ive seen rrated films with male nudity. granted, they only offer some fleeting views, but where are the rrated films with gaping vulvas and flapping labia? nowhere, because they dont exist. the same goes for those crappy cable shows schlongs swinging in the breeze but not a clitoris in sight. and those pretentious indie movies like the brown bunny, in which were treated to the site of vincent gallos throbbing johnson, but not a trace of pink visible on chloe sevigny. before crying or implying doublestandard in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women there are no genitals on display when actresses appears nude, and the same cannot be said for a man. in fact, you generally wont see female genitals in an american film in anything short of porn or explicit erotica. this alleged doublestandard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of womens bodies.\n",
            "Tokenized text (tokens): ['i', 'am', 'curious', 'yellow', 'is', 'a', '[UNK]', 'and', 'pretentious', 'steaming', 'pile', '.', 'it', 'doesnt', 'matter', 'what', 'ones', 'political', 'views', 'are', 'because', 'this', 'film', 'can', 'hardly', 'be', 'taken', 'seriously', 'on', 'any', 'level', '.', 'as', 'for', 'the', 'claim', 'that', 'frontal', 'male', 'nudity', 'is', 'an', 'automatic', '[UNK]', ',', 'that', 'isnt', 'true', '.', 'ive', 'seen', '[UNK]', 'films', 'with', 'male', 'nudity', '.', 'granted', ',', 'they', 'only', 'offer', 'some', 'fleeting', 'views', ',', 'but', 'where', 'are', 'the', '[UNK]', 'films', 'with', '[UNK]', '[UNK]', 'and', '[UNK]', '[UNK]', '?', 'nowhere', ',', 'because', 'they', 'dont', 'exist', '.', 'the', 'same', 'goes', 'for', 'those', 'crappy', 'cable', 'shows', '[UNK]', 'swinging', 'in', 'the', '[UNK]', 'but', 'not', 'a', '[UNK]', 'in', 'sight', '.', 'and', 'those', 'pretentious', 'indie', 'movies', 'like', 'the', 'brown', 'bunny', ',', 'in', 'which', 'were', 'treated', 'to', 'the', 'site', 'of', 'vincent', '[UNK]', '[UNK]', 'johnson', ',', 'but', 'not', 'a', 'trace', 'of', 'pink', 'visible', 'on', 'chloe', '[UNK]', '.', 'before', 'crying', 'or', '[UNK]', '[UNK]', 'in', 'matters', 'of', 'nudity', ',', 'the', 'mentally', '[UNK]', 'should', 'take', 'into', 'account', 'one', '[UNK]', 'obvious', '[UNK]', 'difference', 'between', 'men', 'and', 'women', 'there', 'are', 'no', '[UNK]', 'on', 'display', 'when', 'actresses', 'appears', 'nude', ',', 'and', 'the', 'same', 'cannot', 'be', 'said', 'for', 'a', 'man', '.', 'in', 'fact', ',', 'you', 'generally', 'wont', 'see', 'female', '[UNK]', 'in', 'an', 'american', 'film', 'in', 'anything', 'short', 'of', 'porn', 'or', 'explicit', '[UNK]', '.', 'this', 'alleged', '[UNK]', 'is', 'less', 'a', 'double', 'standard', 'than', 'an', 'admittedly', 'depressing', 'ability', 'to', 'come', 'to', 'terms', '[UNK]', 'with', 'the', '[UNK]', 'of', 'womens', 'bodies', '.']\n",
            "Tokenized text (IDs): [13, 242, 1979, 4427, 10, 7, 1, 6, 1916, 8673, 2459, 4, 12, 151, 557, 51, 519, 1011, 2686, 26, 89, 14, 22, 73, 964, 30, 617, 611, 23, 105, 669, 4, 17, 18, 3, 2254, 15, 7338, 939, 1010, 10, 36, 7982, 1, 5, 15, 213, 285, 4, 198, 112, 1, 101, 19, 939, 1010, 4, 2453, 5, 37, 66, 1433, 49, 8914, 2686, 5, 21, 120, 26, 3, 1, 101, 19, 1, 1, 6, 1, 1, 57, 1254, 5, 89, 37, 95, 1778, 4, 3, 172, 267, 18, 149, 2090, 1933, 268, 1, 7292, 11, 3, 1, 21, 24, 7, 1, 11, 1692, 4, 6, 149, 1916, 2814, 98, 41, 3, 2128, 5294, 5, 11, 65, 71, 1877, 9, 3, 2144, 8, 3192, 1, 1, 2825, 5, 21, 24, 7, 6801, 8, 4966, 4704, 23, 7999, 1, 4, 160, 2542, 43, 1, 1, 11, 2238, 8, 1010, 5, 3, 3031, 1, 145, 194, 85, 2655, 31, 1, 574, 1, 1437, 199, 351, 6, 367, 50, 26, 61, 1, 23, 2399, 55, 1587, 727, 2521, 5, 6, 3, 172, 561, 30, 298, 18, 7, 138, 4, 11, 193, 5, 25, 1201, 482, 70, 664, 1, 11, 36, 320, 22, 11, 231, 357, 8, 1548, 43, 3763, 1, 4, 14, 6619, 1, 10, 329, 7, 1695, 1284, 76, 36, 3372, 2208, 1229, 9, 214, 9, 1269, 1, 19, 3, 1, 8, 4618, 2298, 4]\n",
            "Decoded text: i am curious yellow is a [UNK] and pretentious steaming pile . it doesnt matter what ones political views are because this film can hardly be taken seriously on any level . as for the claim that frontal male nudity is an automatic [UNK] , that isnt true . ive seen [UNK] films with male nudity . granted , they only offer some fleeting views , but where are the [UNK] films with [UNK] [UNK] and [UNK] [UNK] ? nowhere , because they dont exist . the same goes for those crappy cable shows [UNK] swinging in the [UNK] but not a [UNK] in sight . and those pretentious indie movies like the brown bunny , in which were treated to the site of vincent [UNK] [UNK] johnson , but not a trace of pink visible on chloe [UNK] . before crying or [UNK] [UNK] in matters of nudity , the mentally [UNK] should take into account one [UNK] obvious [UNK] difference between men and women there are no [UNK] on display when actresses appears nude , and the same cannot be said for a man . in fact , you generally wont see female [UNK] in an american film in anything short of porn or explicit [UNK] . this alleged [UNK] is less a double standard than an admittedly depressing ability to come to terms [UNK] with the [UNK] of womens bodies .\n",
            "\n",
            "Original text: if only to avoid making this type of film in the future. this film is interesting as an experiment but tells no cogent story.one might feel virtuous for sitting thru it because it touches on so many important issues but it does so without any discernable motive. the viewer comes away with no new perspectives unless one comes up with one while ones mind wanders, as it will invariably do during this pointless film.one might better spend ones time staring out a window at a tree growing.\n",
            "Tokenized text (tokens): ['if', 'only', 'to', 'avoid', 'making', 'this', 'type', 'of', 'film', 'in', 'the', 'future', '.', 'this', 'film', 'is', 'interesting', 'as', 'an', 'experiment', 'but', 'tells', 'no', '[UNK]', 'story', '.', 'one', 'might', 'feel', '[UNK]', 'for', 'sitting', 'thru', 'it', 'because', 'it', 'touches', 'on', 'so', 'many', 'important', 'issues', 'but', 'it', 'does', 'so', 'without', 'any', '[UNK]', 'motive', '.', 'the', 'viewer', 'comes', 'away', 'with', 'no', 'new', 'perspectives', 'unless', 'one', 'comes', 'up', 'with', 'one', 'while', 'ones', 'mind', 'wanders', ',', 'as', 'it', 'will', '[UNK]', 'do', 'during', 'this', 'pointless', 'film', '.', 'one', 'might', 'better', 'spend', 'ones', 'time', 'staring', 'out', 'a', 'window', 'at', 'a', 'tree', 'growing', '.']\n",
            "Tokenized text (IDs): [46, 66, 9, 780, 252, 14, 600, 8, 22, 11, 3, 693, 4, 14, 22, 10, 219, 17, 36, 2813, 21, 701, 61, 1, 68, 4, 31, 237, 240, 1, 18, 1240, 4491, 12, 89, 12, 2407, 23, 40, 111, 681, 1314, 21, 12, 126, 40, 207, 105, 1, 6181, 4, 3, 517, 264, 244, 19, 61, 162, 8304, 893, 31, 264, 62, 19, 31, 137, 519, 346, 5978, 5, 17, 12, 82, 1, 87, 311, 14, 1135, 22, 4, 31, 237, 128, 1115, 519, 63, 4570, 48, 7, 2016, 34, 7, 3010, 1780, 4]\n",
            "Decoded text: if only to avoid making this type of film in the future . this film is interesting as an experiment but tells no [UNK] story . one might feel [UNK] for sitting thru it because it touches on so many important issues but it does so without any [UNK] motive . the viewer comes away with no new perspectives unless one comes up with one while ones mind wanders , as it will [UNK] do during this pointless film . one might better spend ones time staring out a window at a tree growing .\n",
            "\n",
            "Original text: this film was probably inspired by godards masculin, fminin and i urge you to see that film instead.the film has two strong elements and those are, 1 the realistic acting 2 the impressive, undeservedly good, photo. apart from that, what strikes me most is the endless stream of silliness. lena nyman has to be most annoying actress in the world. she acts so stupid and with all the nudity in this film,...its unattractive. comparing to godards film, intellectuality has been replaced with stupidity. without going too far on this subject, i would say that follows from the difference in ideals between the french and the swedish society.a movie of its time, and place. 210.\n",
            "Tokenized text (tokens): ['this', 'film', 'was', 'probably', 'inspired', 'by', '[UNK]', '[UNK]', ',', '[UNK]', 'and', 'i', 'urge', 'you', 'to', 'see', 'that', 'film', 'instead', '.', 'the', 'film', 'has', 'two', 'strong', 'elements', 'and', 'those', 'are', ',', '1', 'the', 'realistic', 'acting', '2', 'the', 'impressive', ',', '[UNK]', 'good', ',', 'photo', '.', 'apart', 'from', 'that', ',', 'what', 'strikes', 'me', 'most', 'is', 'the', 'endless', 'stream', 'of', 'silliness', '.', 'lena', '[UNK]', 'has', 'to', 'be', 'most', 'annoying', 'actress', 'in', 'the', 'world', '.', 'she', 'acts', 'so', 'stupid', 'and', 'with', 'all', 'the', 'nudity', 'in', 'this', 'film', ',...', 'its', 'unattractive', '.', 'comparing', 'to', '[UNK]', 'film', ',', '[UNK]', 'has', 'been', 'replaced', 'with', 'stupidity', '.', 'without', 'going', 'too', 'far', 'on', 'this', 'subject', ',', 'i', 'would', 'say', 'that', 'follows', 'from', 'the', 'difference', 'in', 'ideals', 'between', 'the', 'french', 'and', 'the', 'swedish', 'society', '.', 'a', 'movie', 'of', 'its', 'time', ',', 'and', 'place', '.', '210', '.']\n",
            "Tokenized text (IDs): [14, 22, 16, 241, 1622, 35, 1, 1, 5, 1, 6, 13, 4186, 25, 9, 70, 15, 22, 300, 4, 3, 22, 47, 110, 567, 776, 6, 149, 26, 5, 421, 3, 824, 117, 277, 3, 1141, 5, 1, 53, 5, 5268, 4, 957, 39, 15, 5, 51, 3331, 74, 94, 10, 3, 2140, 7183, 8, 5278, 4, 4641, 1, 47, 9, 30, 94, 613, 520, 11, 3, 188, 4, 58, 1401, 40, 373, 6, 19, 33, 3, 1010, 11, 14, 22, 8516, 32, 6285, 4, 4367, 9, 1, 22, 5, 1, 47, 80, 2924, 19, 2981, 4, 207, 171, 104, 239, 23, 14, 868, 5, 13, 64, 136, 15, 1147, 39, 3, 1437, 11, 7902, 199, 3, 790, 6, 3, 3887, 916, 4, 7, 20, 8, 32, 63, 5, 6, 272, 4, 3695, 4]\n",
            "Decoded text: this film was probably inspired by [UNK] [UNK] , [UNK] and i urge you to see that film instead . the film has two strong elements and those are , 1 the realistic acting 2 the impressive , [UNK] good , photo . apart from that , what strikes me most is the endless stream of silliness . lena [UNK] has to be most annoying actress in the world . she acts so stupid and with all the nudity in this film ,... its unattractive . comparing to [UNK] film , [UNK] has been replaced with stupidity . without going too far on this subject , i would say that follows from the difference in ideals between the french and the swedish society . a movie of its time , and place . 210 .\n",
            "\n",
            "Original text: oh, brother...after hearing about this ridiculous film for umpteen years all i can think of is that old peggy lee song..is that all there is?? ...i was just an early teen when this smoked fish hit the u.s. i was too young to get in the theater although i did manage to sneak into goodbye columbus. then a screening at a local film museum beckoned  finally i could see this film, except now i was as old as my parents were when they schlepped to see it!!the only reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its u.s. release. millions of people flocked to this stinker, thinking they were going to see a sex film...instead, they got lots of closeups of gnarly, repulsive swedes, onstreet interviews in bland shopping malls, asinie political pretension...and feeble whocares simulated sex scenes with saggy, pale actors.cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..but if it werent for the censorship scandal, it would have been ignored, then forgotten.instead, the i am blank, blank rhythymed title was repeated endlessly for years as a titilation for porno films i am curious, lavender  for gay films, i am curious, black  for blaxploitation films, etc.. and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that naughty sex film that revolutionized the film industry...yeesh, avoid like the plague..or if you must see it  rent the video and fast forward to the dirty parts, just to get it over with.\n",
            "Tokenized text (tokens): ['oh', ',', 'brother', '...', 'after', 'hearing', 'about', 'this', 'ridiculous', 'film', 'for', '[UNK]', 'years', 'all', 'i', 'can', 'think', 'of', 'is', 'that', 'old', 'peggy', 'lee', 'song', '..', 'is', 'that', 'all', 'there', 'is', '??', '...', 'i', 'was', 'just', 'an', 'early', 'teen', 'when', 'this', '[UNK]', 'fish', 'hit', 'the', 'u', '.', 's', '.', 'i', 'was', 'too', 'young', 'to', 'get', 'in', 'the', 'theater', 'although', 'i', 'did', 'manage', 'to', 'sneak', 'into', 'goodbye', '[UNK]', '.', 'then', 'a', 'screening', 'at', 'a', 'local', 'film', 'museum', '[UNK]', 'finally', 'i', 'could', 'see', 'this', 'film', ',', 'except', 'now', 'i', 'was', 'as', 'old', 'as', 'my', 'parents', 'were', 'when', 'they', '[UNK]', 'to', 'see', 'it', '!!', 'the', 'only', 'reason', 'this', 'film', 'was', 'not', 'condemned', 'to', 'the', 'anonymous', '[UNK]', 'of', 'time', 'was', 'because', 'of', 'the', '[UNK]', 'case', '[UNK]', 'by', 'its', 'u', '.', 's', '.', 'release', '.', 'millions', 'of', 'people', '[UNK]', 'to', 'this', 'stinker', ',', 'thinking', 'they', 'were', 'going', 'to', 'see', 'a', 'sex', 'film', '...', 'instead', ',', 'they', 'got', 'lots', 'of', 'closeups', 'of', '[UNK]', ',', 'repulsive', '[UNK]', ',', '[UNK]', 'interviews', 'in', 'bland', 'shopping', '[UNK]', ',', '[UNK]', 'political', '[UNK]', '...', 'and', 'feeble', '[UNK]', '[UNK]', 'sex', 'scenes', 'with', '[UNK]', ',', 'pale', 'actors', '.', 'cultural', 'icon', ',', 'holy', 'grail', ',', 'historic', '[UNK]', '..', 'whatever', 'this', 'thing', 'was', ',', 'shred', 'it', ',', 'burn', 'it', ',', 'then', 'stuff', 'the', '[UNK]', 'in', 'a', 'lead', 'box', '!', 'elite', '[UNK]', 'still', '[UNK]', 'to', 'find', 'value', 'in', 'its', 'boring', '[UNK]', 'revolutionary', 'political', '[UNK]', '..', 'but', 'if', 'it', 'werent', 'for', 'the', 'censorship', 'scandal', ',', 'it', 'would', 'have', 'been', 'ignored', ',', 'then', 'forgotten', '.', 'instead', ',', 'the', 'i', 'am', 'blank', ',', 'blank', '[UNK]', 'title', 'was', 'repeated', 'endlessly', 'for', 'years', 'as', 'a', '[UNK]', 'for', 'porno', 'films', 'i', 'am', 'curious', ',', '[UNK]', 'for', 'gay', 'films', ',', 'i', 'am', 'curious', ',', 'black', 'for', 'blaxploitation', 'films', ',', 'etc', '..', 'and', 'every', 'ten', 'years', 'or', 'so', 'the', 'thing', 'rises', 'from', 'the', 'dead', ',', 'to', 'be', 'viewed', 'by', 'a', 'new', 'generation', 'of', '[UNK]', 'who', 'want', 'to', 'see', 'that', 'naughty', 'sex', 'film', 'that', '[UNK]', 'the', 'film', 'industry', '...', '[UNK]', ',', 'avoid', 'like', 'the', 'plague', '..', 'or', 'if', 'you', 'must', 'see', 'it', 'rent', 'the', 'video', 'and', 'fast', 'forward', 'to', 'the', 'dirty', 'parts', ',', 'just', 'to', 'get', 'it', 'over', 'with', '.']\n",
            "Tokenized text (IDs): [452, 5, 603, 79, 106, 2179, 45, 14, 640, 22, 18, 1, 153, 33, 13, 73, 108, 8, 10, 15, 170, 8066, 847, 625, 380, 10, 15, 33, 50, 10, 1830, 79, 13, 16, 44, 36, 403, 1855, 55, 14, 1, 3182, 644, 3, 1242, 4, 826, 4, 13, 16, 104, 187, 9, 81, 11, 3, 741, 258, 13, 122, 1887, 9, 4973, 85, 5941, 1, 4, 97, 7, 2913, 34, 7, 702, 22, 4482, 1, 408, 13, 103, 70, 14, 22, 5, 547, 154, 13, 16, 17, 170, 17, 60, 786, 71, 55, 37, 1, 9, 70, 12, 569, 3, 66, 280, 14, 22, 16, 24, 8550, 9, 3, 9050, 1, 8, 63, 16, 89, 8, 3, 1, 415, 1, 35, 32, 1242, 4, 826, 4, 782, 4, 3118, 8, 86, 1, 9, 14, 4181, 5, 532, 37, 71, 171, 9, 70, 7, 395, 22, 79, 300, 5, 37, 190, 752, 8, 3542, 8, 1, 5, 6193, 1, 5, 1, 2960, 11, 1891, 5833, 1, 5, 1, 1011, 1, 79, 6, 6937, 1, 1, 395, 141, 19, 1, 5, 6337, 157, 4, 2812, 4905, 5, 3950, 9909, 5, 5811, 1, 380, 850, 14, 156, 16, 5, 7699, 12, 5, 3563, 12, 5, 97, 531, 3, 1, 11, 7, 485, 1021, 52, 5863, 1, 130, 1, 9, 167, 1112, 11, 32, 352, 1, 4254, 1011, 1, 380, 21, 46, 12, 1145, 18, 3, 8372, 9006, 5, 12, 64, 28, 80, 3655, 5, 97, 1580, 4, 300, 5, 3, 13, 242, 3778, 5, 3778, 1, 427, 16, 2422, 6240, 18, 153, 17, 7, 1, 18, 4560, 101, 13, 242, 1979, 5, 1, 18, 1006, 101, 5, 13, 242, 1979, 5, 331, 18, 9849, 101, 5, 515, 380, 6, 174, 760, 153, 43, 40, 3, 156, 5166, 39, 3, 362, 5, 9, 30, 2408, 35, 7, 162, 2251, 8, 1, 38, 183, 9, 70, 15, 5423, 395, 22, 15, 1, 3, 22, 1569, 79, 1, 5, 780, 41, 3, 3223, 380, 43, 46, 25, 226, 70, 12, 879, 3, 396, 6, 833, 982, 9, 3, 1630, 521, 5, 44, 9, 81, 12, 131, 19, 4]\n",
            "Decoded text: oh , brother ... after hearing about this ridiculous film for [UNK] years all i can think of is that old peggy lee song .. is that all there is ?? ... i was just an early teen when this [UNK] fish hit the u . s . i was too young to get in the theater although i did manage to sneak into goodbye [UNK] . then a screening at a local film museum [UNK] finally i could see this film , except now i was as old as my parents were when they [UNK] to see it !! the only reason this film was not condemned to the anonymous [UNK] of time was because of the [UNK] case [UNK] by its u . s . release . millions of people [UNK] to this stinker , thinking they were going to see a sex film ... instead , they got lots of closeups of [UNK] , repulsive [UNK] , [UNK] interviews in bland shopping [UNK] , [UNK] political [UNK] ... and feeble [UNK] [UNK] sex scenes with [UNK] , pale actors . cultural icon , holy grail , historic [UNK] .. whatever this thing was , shred it , burn it , then stuff the [UNK] in a lead box ! elite [UNK] still [UNK] to find value in its boring [UNK] revolutionary political [UNK] .. but if it werent for the censorship scandal , it would have been ignored , then forgotten . instead , the i am blank , blank [UNK] title was repeated endlessly for years as a [UNK] for porno films i am curious , [UNK] for gay films , i am curious , black for blaxploitation films , etc .. and every ten years or so the thing rises from the dead , to be viewed by a new generation of [UNK] who want to see that naughty sex film that [UNK] the film industry ... [UNK] , avoid like the plague .. or if you must see it rent the video and fast forward to the dirty parts , just to get it over with .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for idx in range(5):\n",
        "    example = dataset_train[idx]\n",
        "    original_text = example[\"text\"]\n",
        "    tokenized_text = tokenizer.encode(example[\"text\"])\n",
        "    decoded_text = tokenizer.decode(tokenized_text.ids, skip_special_tokens=False)\n",
        "\n",
        "    print(f\"Original text: {original_text}\")\n",
        "    print(f\"Tokenized text (tokens): {tokenized_text.tokens}\")\n",
        "    print(f\"Tokenized text (IDs): {tokenized_text.ids}\")\n",
        "    print(f\"Decoded text: {decoded_text}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLFW9yAy9tGY"
      },
      "source": [
        "We plot the distribution of the sequence lengths in the training set. Later, we can use this to determine the maximum sequence length to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RVSnL6KZ9tGZ",
        "outputId": "b103b409-ac7e-444e-bea9-d05a0c99ce95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Histogram of token lengths\n",
        "token_lengths = [len(tokenizer.encode(text).ids) for text in dataset_train[\"text\"]]\n",
        "median_length = np.median(token_lengths)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "ax.hist(token_lengths, bins=50, color=\"black\")\n",
        "ax.set_xticks(np.arange(0, np.max(token_lengths) + 1, 100))\n",
        "ax.tick_params(axis=\"x\", labelrotation=45)\n",
        "ax.set_xlabel(\"Number of tokens in sequence\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.axvline(median_length, color=\"red\", linestyle=\"--\", label=f\"Median length: {median_length}\")\n",
        "ax.legend()\n",
        "fig.suptitle(\"Histogram of token lengths\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvVQHnSa9tGZ"
      },
      "source": [
        "### 1.4 Dataset and Data Loaders\n",
        "\n",
        "To deal with sequences of different lengths, we implement a simple `IMDBDataset` class that pads the sequences to the same length using the `[PAD]` token, or truncates them if they exceed the maximum sequence length. We also prepend the `[CLS]` token to the input sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pVd-p4BL9tGZ"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length, pad_idx, cls_idx):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.pad_idx = pad_idx\n",
        "        self.cls_idx = cls_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text and prepend the [CLS] token\n",
        "        tokenized_text = self.tokenizer.encode(text)\n",
        "        token_ids = tokenized_text.ids\n",
        "        token_ids = [self.cls_idx] + token_ids # add start\n",
        "\n",
        "\n",
        "        # Pad or truncate the sequence to the maximum length\n",
        "        if len(token_ids) < self.max_length:\n",
        "            token_ids += [self.pad_idx] * (self.max_length - len(token_ids))\n",
        "        else:\n",
        "            token_ids = token_ids[:self.max_length]\n",
        "\n",
        "        return torch.tensor(token_ids), torch.tensor(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV86l2kB9tGZ"
      },
      "source": [
        "We create some constants:\n",
        "\n",
        "- `MAX_LENGTH`: the maximum sequence length\n",
        "- `PAD_ID`: the ID of the `[PAD]` token\n",
        "- `CLS_ID`: the ID of the `[CLS]` token\n",
        "- `BATCH_SIZE`: the batch size\n",
        "- `NUM_WORKERS`: the number of workers for data loading (set this to 0 if you are encountering issues with the DataLoader)\n",
        "\n",
        "We also create the training and testing datasets and data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6LWPVzgz9tGZ"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "MAX_LENGTH = 256\n",
        "PAD_ID = tokenizer.token_to_id(PAD_TOKEN)\n",
        "CLS_ID = tokenizer.token_to_id(CLS_TOKEN)\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# Dataset instances\n",
        "train_dataset = IMDBDataset(dataset_train[\"text\"], dataset_train[\"label\"], tokenizer, MAX_LENGTH, PAD_ID, CLS_ID)\n",
        "val_dataset = IMDBDataset(dataset_val[\"text\"], dataset_val[\"label\"], tokenizer, MAX_LENGTH, PAD_ID, CLS_ID)\n",
        "test_dataset = IMDBDataset(dataset_test[\"text\"], dataset_test[\"label\"], tokenizer, MAX_LENGTH, PAD_ID, CLS_ID)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rvMIYvJ9tGZ"
      },
      "source": [
        "## 2. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOBNL6_P9tGZ"
      },
      "source": [
        "### 2.1 Training Loop\n",
        "\n",
        "We define a simple train function that takes `model` and trains it for `num_epochs` on `train_loader`. We give the `criterion` (loss function) and `optimizer` as arguments to the function. The train function also needs to know `pad_id`, the ID of the `[PAD]` token so we can generate the attention mask making sure the model does not attend to the padding tokens. After each epoch, we evaluate the model on `val_loader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rhfALYpV9tGZ"
      },
      "outputs": [],
      "source": [
        "def create_mask(sequences, pad_id):\n",
        "    \"\"\"\n",
        "    Input shape of token sequences: (batch_size, seq_length)\n",
        "    Output shape: (batch_size, seq_length) boolean mask with True where the padding tokens are located\n",
        "    \"\"\"\n",
        "    # den funker men ser helt dust ut\n",
        "    mask = (sequences == pad_id)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, pad_id):\n",
        "    #Extrem speedup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        correct, total = 0, 0\n",
        "        for sequences, labels in (pbar := tqdm(train_loader)):\n",
        "\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            attention_mask = create_mask(sequences, pad_id)\n",
        "            outputs = model(sequences, mask=attention_mask)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # Clip gradients for stability\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            correct += ((outputs > 0.5) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            pbar.set_description(f\"Epoch {epoch+1}, Train Loss: {np.mean(train_losses):.4f}, Train Acc.: {correct / total:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        #HUSK EVAL\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for sequences, labels in tqdm(val_loader):\n",
        "                sequences = sequences.to(device)\n",
        "                labels = labels.float().to(device)\n",
        "                attention_mask = create_mask(sequences, pad_id)\n",
        "                outputs = model(sequences, mask=attention_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_losses.append(loss.item())\n",
        "                correct += ((outputs > 0.5) == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        print(f\"Epoch {epoch+1}, Val Loss: {np.mean(val_losses):.4f}, Val Acc.: {correct / total:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLGOAcnQ9tGa"
      },
      "source": [
        "### 2.2 Multi-Head Attention\n",
        "\n",
        "We implement the multi-head attention mechanism. The multi-head attention mechanism consists of `num_heads` independent attention mechanisms. We concatenate the outputs of the different heads and project them back to the model's dimension. We use the scaled dot-product attention mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NFx2wneG9tGa"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads):\n",
        "        \"\"\"\n",
        "        Multihead attention module.\n",
        "        Args:\n",
        "            dim: Dimension of the input vectors\n",
        "            num_heads: Number of attention heads\n",
        "            dropout: Dropout rate for attention scores (default: 0.1)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert dim % num_heads == 0, f\"Dimension {dim} must be divisible by num_heads {num_heads}\"\n",
        "\n",
        "        self.head_dim = dim // num_heads\n",
        "\n",
        "        # TODO: Linear transformations for query, key, and value\n",
        "        #straight up bare linear fc\n",
        "        self.query = nn.Linear(self.dim, self.dim)\n",
        "        self.key = nn.Linear(self.dim,self.dim)\n",
        "        self.value = nn.Linear(self.dim,self.dim)\n",
        "\n",
        "        # TODO: Output linear transformation\n",
        "        self.out_proj = nn.Linear(self.dim,self.dim)\n",
        "\n",
        "    #tror initen sitter.\n",
        "    def forward(self, query, key, value, key_padding_mask):\n",
        "\n",
        "        Q = self.query(query)\n",
        "        K = self.key(key)\n",
        "        V = self.value(value)\n",
        "\n",
        "        batch_size, seq_length, _ = query.shape\n",
        "        #Get correct shape\n",
        "        def reshape_heads(x):\n",
        "            return x.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        #Reshape q,k,v\n",
        "        Q = reshape_heads(Q)\n",
        "        K = reshape_heads(K)\n",
        "        V = reshape_heads(V)\n",
        "\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        #Apply mask\n",
        "        if key_padding_mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(key_padding_mask[:, None, None, :] == 1, float('-inf'))\n",
        "\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)  # Normalize scores\n",
        "        attn_output = torch.matmul(attn_probs, V)  # Apply attention to values\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.dim)\n",
        "        output = self.out_proj(attn_output)  # Final linear layer\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFcDYg3v9tGa"
      },
      "source": [
        "### 2.3 Encoder Block\n",
        "\n",
        "We implement the encoder block, which consists of our multi-head attention layer followed by a feedforward neural network. We also add residual connections and layer normalizations as specified in the project description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ha9fF3ze9tGa"
      },
      "outputs": [],
      "source": [
        "def mlp(dim, dropout=0.1):\n",
        "    #Follows the instructions from project\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(dim, 4*dim),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(4*dim, dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.multihead= MultiheadAttention(dim, num_heads)\n",
        "        self.mlp=  mlp(dim)\n",
        "        self.layer_norm1 = nn.LayerNorm(dim)\n",
        "        self.dropout1 =nn.Dropout(dropout)\n",
        "        self.layer_norm2 = nn.LayerNorm(dim)\n",
        "        self.dropout2 =nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        skip= x\n",
        "        x= self.layer_norm1(x)\n",
        "        x= self.multihead.forward(x,x,x,mask)\n",
        "        x = self.dropout1(x)\n",
        "        x= x+skip #Add skip\n",
        "        skip= x\n",
        "        x= self.layer_norm2(x)\n",
        "        x= self.mlp.forward(x)\n",
        "        x = self.dropout2(x)\n",
        "        x= x+skip #Add skip again\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrPUTzwf9tGa"
      },
      "source": [
        "### 2.4 Model Specification\n",
        "\n",
        "We specify our classifier model, which consists of an embedding layer, followed by `num_layers` encoder blocks. We use a linear layer to project the output of the last encoder block and apply sigmoid activation to get the final output.\n",
        "\n",
        "We also define the positional encoding, which is added to the input embeddings to give the model information about the position of the tokens in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GfQ_wanc9tGb"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional encoding module: adds positional information to the input embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, max_len):\n",
        "        super().__init__()\n",
        "        self.pe = torch.zeros(max_len, embed_size)\n",
        "    def forward(self, x):\n",
        "        self.pe = self.pe.to(x.device)\n",
        "        x = x + self.pe[:x.size(1)]\n",
        "        return x\n",
        "\n",
        "class SentimentTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, embedding_dim, num_heads, num_layers, pad_idx, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.pos_encoder = PositionalEncoding(embedding_dim, max_len)\n",
        "\n",
        "        self.encoder = nn.ModuleList([EncoderBlock(embedding_dim, num_heads, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(embedding_dim, 1)  # Output layer\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
        "        x = self.pos_encoder(x)  # Add positional encodings\n",
        "\n",
        "        for encoder in self.encoder:\n",
        "            x = encoder(x, mask=mask)\n",
        "\n",
        "        x = x[:, 0, :]  # Take the first token's embedding (CLS token equivalent)\n",
        "\n",
        "        return self.sigmoid(self.fc(x)).squeeze()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URufJutq9tGb"
      },
      "source": [
        "### 2.5 Training the Model\n",
        "\n",
        "We can now train the model. It is recommended that you use the parameters provided in the cell below as they should work pretty good without requiring too much computational power.\n",
        "\n",
        "Train the model for (at least) 3 epochs. Each epoch should take around 7 to 12 minutes on a modern CPU. If you are training on a laptop, make sure it is plugged in. Moreover, closing all other applications can also help. Take a well-deserved break while the model trains.\n",
        "\n",
        "If you struggle with extremely long training times, you can do one or more of the following to speed up training:\n",
        "\n",
        "- Increase `MIN_FREQUENCY` and re-train the tokenizer to reduce the vocabulary size.\n",
        "- Use a subset of the training data.\n",
        "- Reduce the number of epochs.\n",
        "- Reduce the maximum sequence length.\n",
        "- Experiment with `NUM_WORKERS` in the data loaders (optimal value is system-dependent).\n",
        "- If you have a GPU, modify the training function above to use it.\n",
        "\n",
        "Expect to observe a training accuracy above `0.60` halfway through the first epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GGsh34Nm9tGb",
        "outputId": "8c081b60-e64a-4f79-b397-287322319118",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 1295617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Train Loss: 0.4699, Train Acc.: 0.7670: 100%|| 391/391 [00:23<00:00, 16.69it/s]\n",
            "100%|| 79/79 [00:02<00:00, 33.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Val Loss: 0.3788, Val Acc.: 0.8332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Train Loss: 0.3396, Train Acc.: 0.8534: 100%|| 391/391 [00:21<00:00, 18.56it/s]\n",
            "100%|| 79/79 [00:02<00:00, 33.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Val Loss: 0.3472, Val Acc.: 0.8508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3, Train Loss: 0.2760, Train Acc.: 0.8842: 100%|| 391/391 [00:20<00:00, 18.69it/s]\n",
            "100%|| 79/79 [00:02<00:00, 33.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Val Loss: 0.3691, Val Acc.: 0.8498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4, Train Loss: 0.2317, Train Acc.: 0.9050: 100%|| 391/391 [00:21<00:00, 18.49it/s]\n",
            "100%|| 79/79 [00:02<00:00, 29.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Val Loss: 0.3560, Val Acc.: 0.8480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5, Train Loss: 0.1897, Train Acc.: 0.9255: 100%|| 391/391 [00:20<00:00, 19.27it/s]\n",
            "100%|| 79/79 [00:03<00:00, 23.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Val Loss: 0.3941, Val Acc.: 0.8520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6, Train Loss: 0.1544, Train Acc.: 0.9408: 100%|| 391/391 [00:20<00:00, 18.78it/s]\n",
            "100%|| 79/79 [00:02<00:00, 29.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Val Loss: 0.4949, Val Acc.: 0.8286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7, Train Loss: 0.1238, Train Acc.: 0.9548: 100%|| 391/391 [00:20<00:00, 18.96it/s]\n",
            "100%|| 79/79 [00:02<00:00, 33.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Val Loss: 0.4936, Val Acc.: 0.8390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8, Train Loss: 0.0952, Train Acc.: 0.9659: 100%|| 391/391 [00:20<00:00, 18.68it/s]\n",
            "100%|| 79/79 [00:02<00:00, 32.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Val Loss: 0.6228, Val Acc.: 0.8268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9, Train Loss: 0.0758, Train Acc.: 0.9741: 100%|| 391/391 [00:20<00:00, 18.67it/s]\n",
            "100%|| 79/79 [00:02<00:00, 33.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Val Loss: 0.6826, Val Acc.: 0.8412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10, Train Loss: 0.0534, Train Acc.: 0.9816: 100%|| 391/391 [00:20<00:00, 18.88it/s]\n",
            "100%|| 79/79 [00:02<00:00, 26.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Val Loss: 0.6731, Val Acc.: 0.8370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 10\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-3\n",
        "EMBEDDING_DIM = 96\n",
        "NUM_HEADS = 4\n",
        "NUM_LAYERS = 3\n",
        "\n",
        "model = SentimentTransformer(vocab_size=VOCAB_SIZE,\n",
        "                             max_len=MAX_LENGTH,\n",
        "                             embedding_dim=EMBEDDING_DIM,\n",
        "                             num_heads=NUM_HEADS,\n",
        "                             num_layers=NUM_LAYERS,\n",
        "                             pad_idx=PAD_ID)\n",
        "\n",
        "criterion = nn.BCELoss() # Model output should have sigmoid applied!\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Number of parameters: {n_params}\")\n",
        "\n",
        "train_model(model=model,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            num_epochs=NUM_EPOCHS,\n",
        "            pad_id=PAD_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6tmeEn49tGb"
      },
      "source": [
        "### 2.6 Evaluating the Model on Unseen Data\n",
        "\n",
        "We evaluate the model on the test set and print the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7OxhrE_39tGb",
        "outputId": "9792c76c-76b5-4e03-d38b-0eac075ca2a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 313/313 [00:10<00:00, 30.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (test): 0.8347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, test_loader, pad_id):\n",
        "\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in tqdm(test_loader):\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.float().to(device)\n",
        "            attention_mask = create_mask(sequences, pad_id)\n",
        "            outputs = model(sequences, mask=attention_mask)\n",
        "            predictions = (outputs > 0.5).float()\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "accuracy = evaluate_model(model, test_loader, PAD_ID)\n",
        "print(f\"Accuracy (test): {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KMISkV49tGc"
      },
      "source": [
        "### 2.7 Testing the Model on Custom Examples\n",
        "\n",
        "For fun, we test the model on some custom examples from IMDb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6XlZ8Uzz9tGc"
      },
      "outputs": [],
      "source": [
        "# Function to classify a single review\n",
        "def classify_review(review, model, tokenizer, pad_idx, cls_idx, max_length):\n",
        "    # TODO: Implement this function\n",
        "    # Remember to set the model to evaluation mode, preprocess the review, tokenize it with [CLS] token prepended, pad/truncate and create a mask before passing it to the model\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    review = preprocess_text(review)\n",
        "\n",
        "    tokenized_text = tokenizer.encode(review)\n",
        "    token_ids = tokenized_text.ids\n",
        "    token_ids = [cls_idx] + token_ids\n",
        "    if len(token_ids) < max_length:\n",
        "        token_ids += [pad_idx] * (max_length - len(token_ids))\n",
        "    else:\n",
        "        token_ids = token_ids[:max_length]\n",
        "    sequences = torch.tensor([token_ids]).to(device)  # Create a tensor and move to device\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sequences = sequences.to(device)\n",
        "\n",
        "        attention_mask = create_mask(sequences, pad_idx)\n",
        "        outputs = model(sequences, mask=attention_mask)\n",
        "\n",
        "        sentiment = (outputs > 0.5).float()\n",
        "        p = outputs.item()\n",
        "\n",
        "\n",
        "    return sentiment, p\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "014fwn609tGc"
      },
      "outputs": [],
      "source": [
        "# TODO: Classify some reviews chosen by you\n",
        "first=classify_review(review=\"This is some of the weirdest shit I have ever seen! Really impressed\",model=model, tokenizer=\n",
        "                tokenizer ,pad_idx=PAD_ID,cls_idx=CLS_ID,max_length=256)\n",
        "second= classify_review(review=\"This is some of the weirdest shit I have ever seen! SUCKED BALLS\",model=model, tokenizer=\n",
        "                tokenizer ,pad_idx=PAD_ID,cls_idx=CLS_ID,max_length=256)\n",
        "third =classify_review(review=\"Wow, I dont really know how to feel about this movie. Its not bad, but not good\",model=model, tokenizer=\n",
        "                tokenizer ,pad_idx=PAD_ID,cls_idx=CLS_ID,max_length=256)\n",
        "fourth =classify_review(review=\"WORST MOVIE EVER! I will never watch it again, truly bad. Horrible movie\",model=model, tokenizer=\n",
        "                tokenizer ,pad_idx=PAD_ID,cls_idx=CLS_ID,max_length=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd5bPCiO9tGc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(first)\n",
        "print(second)\n",
        "print(third)\n",
        "print(fourth)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7k719ufJ6Tex",
        "outputId": "31c7128d-b469-4d78-a859-e368c5b0537a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor(1., device='cuda:0'), 0.9998987913131714)\n",
            "(tensor(1., device='cuda:0'), 0.8892558813095093)\n",
            "(tensor(0., device='cuda:0'), 0.05542749539017677)\n",
            "(tensor(0., device='cuda:0'), 0.003785551292821765)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9DPwd40r6VyD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}