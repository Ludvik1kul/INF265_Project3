{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Avoids warning from tokenizer when using num_workers > 0 in DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-only Transformer for Text Sentiment Classification\n",
    "\n",
    "## 1. Data\n",
    "\n",
    "### 1.1 Load the IMDB dataset\n",
    "\n",
    "We will use the IMDB dataset for sentiment classification. The dataset consists of 50,000 movie reviews, each labeled as positive or negative. We will use 25k reviews for training, 5k for validation, and 20k for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED = 420\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "# Load IMDB dataset\n",
    "dataset_train = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset_test = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "# Split test into test and validation\n",
    "dataset_test = dataset_test.train_test_split(test_size=1-VAL_SIZE, seed=SEED)\n",
    "dataset_test, dataset_val = dataset_test[\"test\"], dataset_test[\"train\"]\n",
    "\n",
    "print(f\"Train size: {len(dataset_train)}\")\n",
    "print(f\"Validation size: {len(dataset_val)}\")\n",
    "print(f\"Test size: {len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess the data\n",
    "\n",
    "We remove HTML tags, special characters, and convert the text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    # Remove HTML tags\n",
    "    return re.sub(r'<[^>]*>', '', text)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Remove special characters except for ,.!? and space\n",
    "    return re.sub(r'[^a-zA-Z0-9.,!? ]', '', text)\n",
    "\n",
    "def to_lowercase(text):\n",
    "    # Convert text to lowercase\n",
    "    return text.lower()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Apply all preprocessing steps to the text\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = to_lowercase(text)\n",
    "    return text\n",
    "\n",
    "def preprocess_batch(examples):\n",
    "    # Apply preprocessing to all texts in the batch\n",
    "    examples[\"text\"] = [preprocess_text(text) for text in examples[\"text\"]]\n",
    "    return examples\n",
    "\n",
    "# Preprocess the dataset\n",
    "dataset_train = dataset_train.map(preprocess_batch, batched=True)\n",
    "dataset_val = dataset_val.map(preprocess_batch, batched=True)\n",
    "dataset_test = dataset_test.map(preprocess_batch, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization\n",
    "\n",
    "We use a simple word-level tokenizer to tokenize the text data. We use three special tokens: `[PAD]`, `[UNK]` and `[CLS]`. The `[PAD]` token will be used to pad the input sequences to the same length. The `[UNK]` token is used to represent out-of-vocabulary words (rare words). When classifying text, we will prepend the `[CLS]` token to the input sequence and use its output as the representation of the whole sequence.\n",
    "\n",
    "To reduce the vocabulary size, we only keep words appearing at least `MIN_FREQUENCY` times in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens (padding, unknown, classification)\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "MIN_FREQUENCY = 10\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))\n",
    "tokenizer.pre_tokenizer = Whitespace() # Split on whitespace\n",
    "\n",
    "# Train tokenizer on the training set\n",
    "trainer = WordLevelTrainer(special_tokens=[PAD_TOKEN, UNK_TOKEN, CLS_TOKEN], min_frequency=MIN_FREQUENCY, vocab_size=VOCAB_SIZE)\n",
    "tokenizer.train_from_iterator(dataset_train[\"text\"], trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the tokenizer on a few examples to verify that it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5):\n",
    "    example = dataset_train[idx]\n",
    "    original_text = example[\"text\"]\n",
    "    tokenized_text = tokenizer.encode(example[\"text\"])\n",
    "    decoded_text = tokenizer.decode(tokenized_text.ids, skip_special_tokens=False)\n",
    "\n",
    "    print(f\"Original text: {original_text}\")\n",
    "    print(f\"Tokenized text (tokens): {tokenized_text.tokens}\")\n",
    "    print(f\"Tokenized text (IDs): {tokenized_text.ids}\")\n",
    "    print(f\"Decoded text: {decoded_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the distribution of the sequence lengths in the training set. Later, we can use this to determine the maximum sequence length to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of token lengths\n",
    "token_lengths = [len(tokenizer.encode(text).ids) for text in dataset_train[\"text\"]]\n",
    "median_length = np.median(token_lengths)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(token_lengths, bins=50, color=\"black\")\n",
    "ax.set_xticks(np.arange(0, np.max(token_lengths) + 1, 100))\n",
    "ax.tick_params(axis=\"x\", labelrotation=45)\n",
    "ax.set_xlabel(\"Number of tokens in sequence\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.axvline(median_length, color=\"red\", linestyle=\"--\", label=f\"Median length: {median_length}\")\n",
    "ax.legend()\n",
    "fig.suptitle(\"Histogram of token lengths\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Dataset and Data Loaders\n",
    "\n",
    "To deal with sequences of different lengths, we implement a simple `IMDBDataset` class that pads the sequences to the same length using the `[PAD]` token, or truncates them if they exceed the maximum sequence length. We also prepend the `[CLS]` token to the input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length, pad_idx, cls_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pad_idx = pad_idx\n",
    "        self.cls_idx = cls_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Implement this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some constants:\n",
    "\n",
    "- `MAX_LENGTH`: the maximum sequence length\n",
    "- `PAD_ID`: the ID of the `[PAD]` token\n",
    "- `CLS_ID`: the ID of the `[CLS]` token\n",
    "- `BATCH_SIZE`: the batch size\n",
    "- `NUM_WORKERS`: the number of workers for data loading (set this to 0 if you are encountering issues with the DataLoader)\n",
    "\n",
    "We also create the training and testing datasets and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_LENGTH = 256\n",
    "PAD_ID = tokenizer.token_to_id(PAD_TOKEN)\n",
    "CLS_ID = tokenizer.token_to_id(CLS_TOKEN)\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Dataset instances\n",
    "train_dataset = IMDBDataset(dataset_train[\"text\"], dataset_train[\"label\"], tokenizer, MAX_LENGTH, PAD_ID, CLS_ID)\n",
    "val_dataset = IMDBDataset(dataset_val[\"text\"], dataset_val[\"label\"], tokenizer, MAX_LENGTH, PAD_ID, CLS_ID)\n",
    "test_dataset = IMDBDataset(dataset_test[\"text\"], dataset_test[\"label\"], tokenizer, MAX_LENGTH, PAD_ID, CLS_ID)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training Loop\n",
    "\n",
    "We define a simple train function that takes `model` and trains it for `num_epochs` on `train_loader`. We give the `criterion` (loss function) and `optimizer` as arguments to the function. The train function also needs to know `pad_id`, the ID of the `[PAD]` token so we can generate the attention mask making sure the model does not attend to the padding tokens. After each epoch, we evaluate the model on `val_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(sequences, pad_id):\n",
    "    \"\"\"\n",
    "    Input shape of token sequences: (batch_size, seq_length)\n",
    "    Output shape: (batch_size, seq_length) boolean mask with True where the padding tokens are located\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, pad_id):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        correct, total = 0, 0\n",
    "        for sequences, labels in (pbar := tqdm(train_loader)):\n",
    "            labels = labels.float()\n",
    "            optimizer.zero_grad()\n",
    "            attention_mask = create_mask(sequences, pad_id)\n",
    "            outputs = model(sequences, mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # Clip gradients for stability\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            correct += ((outputs > 0.5) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            pbar.set_description(f\"Epoch {epoch+1}, Train Loss: {np.mean(train_losses):.4f}, Train Acc.: {correct / total:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in tqdm(val_loader):\n",
    "                labels = labels.float()\n",
    "                attention_mask = create_mask(sequences, pad_id)\n",
    "                outputs = model(sequences, mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_losses.append(loss.item())\n",
    "                correct += ((outputs > 0.5) == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {np.mean(val_losses):.4f}, Val Acc.: {correct / total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multi-Head Attention\n",
    "\n",
    "We implement the multi-head attention mechanism. The multi-head attention mechanism consists of `num_heads` independent attention mechanisms. We concatenate the outputs of the different heads and project them back to the model's dimension. We use the scaled dot-product attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        \"\"\"\n",
    "        Multihead attention module.\n",
    "        Args:\n",
    "            dim: Dimension of the input vectors\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Dropout rate for attention scores (default: 0.1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert dim % num_heads == 0, f\"Dimension {dim} must be divisible by num_heads {num_heads}\"\n",
    "\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        # TODO: Linear transformations for query, key, and value\n",
    "        self.query = ...\n",
    "        self.key = ...\n",
    "        self.value = ...\n",
    "\n",
    "        # TODO: Output linear transformation\n",
    "        self.out_proj = ...\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask):\n",
    "        # TODO: Implement this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoder Block\n",
    "\n",
    "We implement the encoder block, which consists of our multi-head attention layer followed by a feedforward neural network. We also add residual connections and layer normalizations as specified in the project description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # TODO: Implement this method\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # TODO: Implement this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Model Specification\n",
    "\n",
    "We specify our classifier model, which consists of an embedding layer, followed by `num_layers` encoder blocks. We use a linear layer to project the output of the last encoder block and apply sigmoid activation to get the final output.\n",
    "\n",
    "We also define the positional encoding, which is added to the input embeddings to give the model information about the position of the tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding module: adds positional information to the input embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, max_len):\n",
    "        super().__init__()\n",
    "        # TODO: Implement this method\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement this method (add positional encodings to the input embeddings x)\n",
    "\n",
    "class SentimentTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, embedding_dim, num_heads, num_layers, pad_idx, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, max_len)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([EncoderBlock(embedding_dim, num_heads, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, 1)  # Output layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        x = self.pos_encoder(x)  # Add positional encodings\n",
    "\n",
    "        for encoder in self.encoder:\n",
    "            x = encoder(x, mask=mask)\n",
    "\n",
    "        x = x[:, 0, :]  # Take the first token's embedding (CLS token equivalent)\n",
    "\n",
    "        return self.sigmoid(self.fc(x)).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the Model\n",
    "\n",
    "We can now train the model. It is recommended that you use the parameters provided in the cell below as they should work pretty good without requiring too much computational power.\n",
    "\n",
    "Train the model for (at least) 3 epochs. Each epoch should take around 7 to 12 minutes on a modern CPU. If you are training on a laptop, make sure it is plugged in. Moreover, closing all other applications can also help. Take a well-deserved break while the model trains.\n",
    "\n",
    "If you struggle with extremely long training times, you can do one or more of the following to speed up training:\n",
    "\n",
    "- Increase `MIN_FREQUENCY` and re-train the tokenizer to reduce the vocabulary size.\n",
    "- Use a subset of the training data.\n",
    "- Reduce the number of epochs.\n",
    "- Reduce the maximum sequence length.\n",
    "- Experiment with `NUM_WORKERS` in the data loaders (optimal value is system-dependent).\n",
    "- If you have a GPU, modify the training function above to use it.\n",
    "\n",
    "Expect to observe a training accuracy above `0.60` halfway through the first epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-3\n",
    "EMBEDDING_DIM = 96\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "model = SentimentTransformer(vocab_size=VOCAB_SIZE, \n",
    "                             max_len=MAX_LENGTH,\n",
    "                             embedding_dim=EMBEDDING_DIM, \n",
    "                             num_heads=NUM_HEADS, \n",
    "                             num_layers=NUM_LAYERS, \n",
    "                             pad_idx=PAD_ID)\n",
    "\n",
    "criterion = nn.BCELoss() # Model output should have sigmoid applied!\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {n_params}\")\n",
    "\n",
    "train_model(model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            pad_id=PAD_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Evaluating the Model on Unseen Data\n",
    "\n",
    "We evaluate the model on the test set and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, pad_id):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in tqdm(test_loader):\n",
    "            attention_mask = create_mask(sequences, pad_id)\n",
    "            outputs = model(sequences, mask=attention_mask)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "accuracy = evaluate_model(model, test_loader, PAD_ID)\n",
    "print(f\"Accuracy (test): {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Testing the Model on Custom Examples\n",
    "\n",
    "For fun, we test the model on some custom examples from IMDb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify a single review\n",
    "def classify_review(review, model, tokenizer, pad_idx, cls_idx, max_length):\n",
    "    # TODO: Implement this function\n",
    "    # Remember to set the model to evaluation mode, preprocess the review, tokenize it with [CLS] token prepended, pad/truncate and create a mask before passing it to the model\n",
    "    return sentiment, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Classify some reviews chosen by you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
